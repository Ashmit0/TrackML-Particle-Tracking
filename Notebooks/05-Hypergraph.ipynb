{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7191156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version 2.6.0\n",
      "PyG version 2.6.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import ortho_group\n",
    "\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.linalg import norm \n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, ReLU, BatchNorm1d, Module, Sequential\n",
    "from torch import Tensor\n",
    "\n",
    "# torch.set_default_dtype(torch.float64)\n",
    "\n",
    "from torch_geometric.typing import (\n",
    "    Adj,\n",
    "    OptPairTensor,\n",
    "    OptTensor,\n",
    "    Size,\n",
    "    SparseTensor,\n",
    "    torch_sparse,\n",
    ")\n",
    "\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import Batch\n",
    "import torch_geometric.transforms as T\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch_geometric.utils import remove_self_loops, to_dense_adj, dense_to_sparse, is_undirected , to_undirected, contains_self_loops , to_networkx , softmax \n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import MessagePassing, global_mean_pool, knn_graph , HypergraphConv\n",
    "from torch_geometric.datasets import QM9\n",
    "from torch_scatter import scatter_add\n",
    "# from torch_scatter import scatter\n",
    "# from torch_cluster import knn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "# import uproot\n",
    "import vector\n",
    "vector.register_awkward()\n",
    "import awkward as ak\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "print(\"PyTorch version {}\".format(torch.__version__))\n",
    "print(\"PyG version {}\".format(torch_geometric.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1b9758b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrackML.Embedding.dataset import PointCloudData\n",
    "from TrackML.Models.utils import buildMLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "377aadfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 5 \n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57686907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hyperedges(node_features: Tensor, radius_threshold: float, batch_size: int = 32) -> Tuple[Tensor, int]:\n",
    "    \"\"\"\n",
    "    Generates hyperedge data for input to a hyperconv layer using the radius cluster algorithm.\n",
    "\n",
    "    Args:\n",
    "        node_features (Tensor): Node features as a PyTorch tensor of shape [num_nodes, num_features].\n",
    "        radius_threshold (float): Radius threshold for clustering nodes into hyperedges.\n",
    "        batch_size (int): Batch size for processing distances.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Tensor, int]: A tuple containing the edge_index tensor and the number of hyperedges.\n",
    "    \"\"\"\n",
    "    num_nodes = node_features.size(0)\n",
    "    hyperedges = []\n",
    "\n",
    "    # Generate hyperedges using batching\n",
    "    for i in range(0, num_nodes, batch_size):\n",
    "        batch_end = min(i + batch_size, num_nodes)\n",
    "        batch_features = node_features[i:batch_end]  # Shape: [batch_size, num_features]\n",
    "\n",
    "        # Compute distances for the batch\n",
    "        distances = torch.cdist(batch_features, node_features)  # Shape: [batch_size, num_nodes]\n",
    "\n",
    "        # Identify nodes within the radius threshold\n",
    "        for j, distance_row in enumerate(distances):\n",
    "            hyperedge = torch.where(distance_row <= radius_threshold)[0]\n",
    "            hyperedges.append(hyperedge)\n",
    "\n",
    "    # Convert hyperedges to edge_index format\n",
    "    edge_index = []\n",
    "    for idx, hyperedge in enumerate(hyperedges):\n",
    "        for node in hyperedge:\n",
    "            edge_index.append([node.item(), idx])\n",
    "\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    return edge_index, len(hyperedges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77a0fcfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hyperedges: 100\n",
      "Edge index shape: torch.Size([2, 142])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_nodes = 100\n",
    "num_features = 3\n",
    "node_features = torch.rand((num_nodes, num_features))\n",
    "radius_threshold = 0.1\n",
    "\n",
    "# Example usage\n",
    "edge_index , num_hg = generate_hyperedges(node_features, radius_threshold)\n",
    "print(f\"Number of hyperedges: {num_hg}\")\n",
    "print(f\"Edge index shape: {edge_index.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1d3ac70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1, 56, 76,  2,  3,  4,  5,  6,  7],\n",
      "        [ 0,  1,  1,  1,  2,  3,  4,  5,  6,  7]])\n"
     ]
    }
   ],
   "source": [
    "print(edge_index[  : , :10  ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15165f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 100, 100]), tensor(142.))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_matrix = to_dense_adj(edge_index)\n",
    "dense_matrix.shape , torch.sum( dense_matrix )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e11011cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coustom hypergraph message passing layer with attention score outputs : \n",
    "\n",
    "class HypergraphConvWithAttention(nn.Module):\n",
    "    def __init__(self, in_node_dim, in_edge_dim, out_dim, use_bias=True):\n",
    "        super().__init__()\n",
    "        self.node_proj = Linear(in_node_dim, out_dim, bias=use_bias)\n",
    "        self.edge_proj = Linear(in_edge_dim, out_dim, bias=use_bias)\n",
    "        self.att_proj = Linear(out_dim, 1, bias=use_bias)\n",
    "        self.out_proj = Linear(out_dim, out_dim, bias=use_bias)\n",
    "\n",
    "    def forward(self, x, e, hyperedge_index):\n",
    "        '''\n",
    "        x : Node features [N, in_node_dim]\n",
    "        e : Hyperedge features [M, in_edge_dim]\n",
    "        hyperedge_index : Edge index [2, E] where E is the number of edges\n",
    "        output : Node features [N, out_dim] and attention weights [E]\n",
    "        '''\n",
    "        row, col = hyperedge_index  # node idx (i), hyperedge idx (j)\n",
    "\n",
    "        #Project node and hyperedge features\n",
    "        x_i = self.node_proj(x[row])       # [E, H]\n",
    "        e_j = self.edge_proj(e[col])       # [E, H]\n",
    "\n",
    "        #Compute attention weights (tanh + projection)\n",
    "        att_input = torch.tanh(x_i + e_j)  # [E, H]\n",
    "        att_scores = self.att_proj(att_input).squeeze(-1)  # [E]\n",
    "\n",
    "        #Normalize attention weights per node i.e \n",
    "        #   across hyperedges per node\n",
    "        att_weights = softmax(att_scores, index=row)       # [E]\n",
    "        \n",
    "        #Weighted message passing from hyperedges to nodes\n",
    "        messages = e_j * att_weights.unsqueeze(-1)         # [E, H]\n",
    "        node_updates = scatter_add(messages, row, dim=0, dim_size=x.size(0))  # [N, H]\n",
    "\n",
    "        # Optional: post-transform (e.g. residual or output layer)\n",
    "        out = self.out_proj(node_updates)  # [N, H]\n",
    "        return out, att_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fe712ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HypergraphConvModelWithAttentionOut(nn.Module):\n",
    "    \"\"\"\n",
    "    Builds a PyG Sequential model with HypergraphConvWithAttention layers,\n",
    "    returning the final node output and attention weights from the last HGConv.\n",
    "\n",
    "    Args:\n",
    "        in_node_dim (int): Input node feature dimension.\n",
    "        in_edge_dim (int): Input hyperedge feature dimension.\n",
    "        hidden_dims (list[int]): List of hidden dimensions for each HGConv layer.\n",
    "        out_dim (int): Output feature dimension for final node prediction.\n",
    "\n",
    "    Returns:\n",
    "        model: nn.Module that returns (node_output, last_attention_weights)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self , \n",
    "        in_node_dim: int, in_edge_dim: int, \n",
    "        hidden_dims: list[int], out_dim: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.activations = nn.ModuleList()\n",
    "\n",
    "        dims = [in_node_dim] + hidden_dims\n",
    "        for i in range(len(hidden_dims)):\n",
    "            self.layers.append(HypergraphConvWithAttention(dims[i], in_edge_dim, dims[i+1]))\n",
    "            self.activations.append(nn.ReLU())\n",
    "\n",
    "        self.final_proj = HypergraphConvWithAttention( dims[-1] , in_edge_dim, out_dim )\n",
    "\n",
    "    def forward(self, x, e, hyperedge_index):\n",
    "        for conv, act in zip(self.layers, self.activations):\n",
    "            x, _ = conv(x, e, hyperedge_index)\n",
    "            x = act(x)\n",
    "        \n",
    "        return self.final_proj(x , e, hyperedge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f95b7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def buildHGConv( insize:int, outsize:int, features:list , dropout : bool )->Tensor: \n",
    "#     layers = [] \n",
    "#     layers.append((HypergraphConv( insize , features[0] , dropout=dropout) , 'x, hyperedge_index -> x') )\n",
    "#     layers.append( nn.ReLU(inplace=True) )\n",
    "#     for i in range( 1 , len( features ) ): \n",
    "#         layers.append(( HypergraphConv( features[i-1] , features[i]  , dropout=dropout) , 'x, hyperedge_index -> x') )\n",
    "#         layers.append( nn.ReLU(inplace=True) )\n",
    "#     layers.append((HypergraphConv(features[-1],outsize,dropout=dropout) , 'x, hyperedge_index -> x') )\n",
    "#     return torch_geometric.nn.Sequential('x, hyperedge_index' , layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "591c3193",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HGNN(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_insize: int,\n",
    "        node_outsize: int,\n",
    "        node_features: list,\n",
    "        hpr_edge_outsize: int,\n",
    "        hpr_edge_features: list ,\n",
    "        hg_outsize: int,\n",
    "        hg_features: list\n",
    "    ):\n",
    "        super(HGNN, self).__init__()\n",
    "        # generate mlp layer for node embedding for hypergraph structure \n",
    "        self.mlp_node = buildMLP(node_insize, node_outsize, node_features)\n",
    "        \n",
    "        # generate mlp to get embedd hyperedge features\n",
    "        self.mlp_hyperedge = buildMLP(node_insize, hpr_edge_outsize, hpr_edge_features)\n",
    "        \n",
    "        # generate hyergraphconvolution network for the hypergraph structure\n",
    "        self.hgconv = HypergraphConvModelWithAttentionOut(\n",
    "            in_node_dim=node_outsize + node_insize,\n",
    "            in_edge_dim=hpr_edge_outsize + node_insize, \n",
    "            hidden_dims=hg_features,\n",
    "            out_dim=hg_outsize\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor: \n",
    "        # embedd node features\n",
    "        node_embedding = self.mlp_node(x)\n",
    "        # generate hyperedge structure form these node embeddings \n",
    "        edge_index , _  = generate_hyperedges( node_embedding , radius_threshold=0.1)\n",
    "        row , col = edge_index\n",
    "        \n",
    "        # generate hyperedge features\n",
    "        # 1. get the hyperedge features from the node features via mean pooling\n",
    "        hyperedge_features = global_mean_pool(x[row], edge_index[1])\n",
    "        # embeddd hyperedge features\n",
    "        hyperedge_features_embdd = self.mlp_hyperedge(hyperedge_features)\n",
    "        \n",
    "        # number of nodes and number of hyperedges\n",
    "        N = x.shape[0]\n",
    "        M = hyperedge_features.shape[0]\n",
    "        \n",
    "        # append original featuers to the hyperedge features\n",
    "        hyperedge_features = torch.cat([hyperedge_features, hyperedge_features_embdd], dim=-1)\n",
    "        # append original features to the node features\n",
    "        x = torch.cat([x, node_embedding], dim=-1)\n",
    "        \n",
    "        # run the hypergraph convolution layer\n",
    "        _ , att_weights = self.hgconv(\n",
    "            x = x , e = hyperedge_features, hyperedge_index = edge_index\n",
    "        )\n",
    "        \n",
    "        # get the node to edge scores\n",
    "        node_to_edge_scores = torch.zeros(N , M)\n",
    "        node_to_edge_scores[row, col] = att_weights\n",
    "    \n",
    "        return node_to_edge_scores  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5feb4fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 8])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(100, 3)  # Example input tensor\n",
    "y = torch.randn( 100 ,5 )\n",
    "torch.cat([x, y], dim=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6616476c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 100])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = HGNN(\n",
    "    node_insize=3,\n",
    "    node_outsize=5,\n",
    "    node_features=[10, 20],\n",
    "    hpr_edge_outsize=5,\n",
    "    hpr_edge_features=[10, 20],\n",
    "    hg_outsize=5,\n",
    "    hg_features=[10, 20]\n",
    ")\n",
    "\n",
    "model_example_out = model( node_features )\n",
    "model_example_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04033b5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(492)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(model_example_out == 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29e822e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EventLossFunction(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(EventLossFunction, self).__init__()\n",
    "#         self.bce_loss = nn.BCELoss()\n",
    "\n",
    "#     def forward(self, softmax_output: Tensor, labels: Tensor) -> Tensor:\n",
    "#         \"\"\"\n",
    "#         Compute the binary cross-entropy loss for the given softmax output and labels.\n",
    "\n",
    "#         Args:\n",
    "#             softmax_output (Tensor): Softmax output of shape (num_nodes, num_hyperedges).\n",
    "#             labels (Tensor): Labels of shape (num_nodes).\n",
    "\n",
    "#         Returns:\n",
    "#             Tensor: Computed loss.\n",
    "#         \"\"\"\n",
    "#         num_nodes, _  = softmax_output.shape\n",
    "\n",
    "#         # Compute pairwise probabilities\n",
    "#         pairwise_probs = torch.matmul(softmax_output, softmax_output.T)  # Shape: (num_nodes, num_nodes)\n",
    "\n",
    "#         # Compute pairwise label agreement\n",
    "#         pairwise_labels = (labels.unsqueeze(0) == labels.unsqueeze(1)).float()  # Shape: (num_nodes, num_nodes)\n",
    "\n",
    "#         # Mask diagonal (self-loops) to avoid self-comparison\n",
    "#         mask = torch.eye(num_nodes, device=softmax_output.device)\n",
    "#         pairwise_probs = pairwise_probs * (1 - mask)\n",
    "#         pairwise_labels = pairwise_labels * (1 - mask)\n",
    "\n",
    "#         # Flatten and compute BCE loss\n",
    "#         loss = self.bce_loss(pairwise_probs.flatten(), pairwise_labels.flatten())\n",
    "\n",
    "#         return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0d69c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventLossFunction(nn.Module):\n",
    "    def __init__(self, batch_size: int = 1024):\n",
    "        \"\"\"\n",
    "        Initialize the EventLossFunction with batching support.\n",
    "\n",
    "        Args:\n",
    "            batch_size (int): The size of the batches to process pairwise computations.\n",
    "        \"\"\"\n",
    "        super(EventLossFunction, self).__init__()\n",
    "        self.bce_loss = nn.BCELoss(reduction='sum')  # Use 'sum' to accumulate loss\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def forward(self, softmax_output: Tensor, labels: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Compute the binary cross-entropy loss for the given softmax output and labels using batching.\n",
    "\n",
    "        Args:\n",
    "            softmax_output (Tensor): Softmax output of shape (num_nodes, num_hyperedges).\n",
    "            labels (Tensor): Labels of shape (num_nodes).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Computed loss as a single scalar tensor.\n",
    "        \"\"\"\n",
    "        num_nodes, _ = softmax_output.shape\n",
    "        device = softmax_output.device\n",
    "\n",
    "        # Initialize loss accumulator\n",
    "        total_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "        # Compute pairwise probabilities and labels in batches\n",
    "        for i in range(0, num_nodes, self.batch_size):\n",
    "            end_i = min(i + self.batch_size, num_nodes)\n",
    "            softmax_batch = softmax_output[i:end_i]  # Shape: (batch_size, num_hyperedges)\n",
    "\n",
    "            # Pairwise probabilities for the current batch\n",
    "            pairwise_probs = torch.matmul(softmax_batch, softmax_output.T)  # Shape: (batch_size, num_nodes)\n",
    "\n",
    "            # Pairwise label agreement for the current batch\n",
    "            pairwise_labels = (labels[i:end_i].unsqueeze(1) == labels.unsqueeze(0)).float()  # Shape: (batch_size, num_nodes)\n",
    "\n",
    "            # Mask diagonal (self-loops) to avoid self-comparison\n",
    "            mask = torch.eye(end_i - i, num_nodes, device=device)\n",
    "            pairwise_probs = pairwise_probs * (1 - mask)\n",
    "            pairwise_labels = pairwise_labels * (1 - mask)\n",
    "\n",
    "            # Flatten and compute BCE loss for the current batch\n",
    "            batch_loss = self.bce_loss(pairwise_probs.flatten(), pairwise_labels.flatten())\n",
    "            total_loss += batch_loss\n",
    "\n",
    "        # Normalize the total loss by the number of pairs\n",
    "        total_pairs = torch.tensor( num_nodes * (num_nodes - 1) )  # Total number of valid pairs\n",
    "        return total_loss / total_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd05a7be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1, 0, 5, 8, 4, 9, 9, 6, 6, 1, 3, 3, 5, 0, 7, 9, 0, 8, 7, 7, 9, 1, 0,\n",
       "        3, 2, 7, 6, 8, 6, 3, 2, 0, 7, 7, 3, 7, 2, 7, 7, 5, 3, 9, 1, 6, 0, 5, 1,\n",
       "        9, 0, 7, 4, 1, 2, 8, 4, 9, 0, 9, 3, 8, 8, 7, 8, 9, 1, 0, 9, 3, 7, 1, 8,\n",
       "        3, 4, 9, 3, 6, 4, 8, 5, 9, 7, 2, 5, 1, 7, 3, 7, 8, 7, 2, 8, 5, 5, 3, 1,\n",
       "        3, 0, 3, 4])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_label = torch.randint(low=0, high=10, size=(100,))\n",
    "random_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e7e40ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4746, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = EventLossFunction( )\n",
    "loss( model_example_out , random_label )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22deefa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_disjoint_hypergraph(softmax_output: Tensor) -> dict:\n",
    "    \"\"\"\n",
    "    Create a disjoint hypergraph by assigning each node to its most probable hyperedge.\n",
    "\n",
    "    Args:\n",
    "        softmax_output (Tensor): Softmax output of shape (num_nodes, num_hyperedges).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are hyperedge indices and values are tensors of node indices.\n",
    "    \"\"\"\n",
    "    # Find the most probable hyperedge for each node\n",
    "    most_probable_hyperedges = torch.argmax(softmax_output, dim=-1)\n",
    "    # Create a disjoint hypergraph by grouping nodes based on their assigned hyperedges\n",
    "    disjoint_hypergraph = {hyperedge.item(): [] for hyperedge in most_probable_hyperedges.unique()}\n",
    "    for node, hyperedge in enumerate(most_probable_hyperedges):\n",
    "        disjoint_hypergraph[hyperedge.item()].append(node)\n",
    "\n",
    "    # Convert the disjoint hypergraph to a more readable format\n",
    "    disjoint_hypergraph = {key: torch.tensor(value) for key, value in disjoint_hypergraph.items()}\n",
    "\n",
    "    return disjoint_hypergraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "427a0294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: tensor([12, 28, 33, 46]), 4: tensor([87]), 5: tensor([47]), 6: tensor([45, 57, 64]), 14: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  9, 10, 11, 14, 15, 16, 17, 18, 19, 20,\n",
      "        21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 34, 35, 36, 38, 39, 40, 41,\n",
      "        42, 43, 44, 48, 49, 50, 52, 53, 54, 55, 56, 58, 59, 60, 61, 62, 63, 65,\n",
      "        66, 67, 68, 69, 71, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n",
      "        86, 88, 89, 90, 91, 93, 94, 95, 96, 97, 98, 99]), 19: tensor([ 8, 13, 37]), 23: tensor([51, 92]), 30: tensor([72]), 88: tensor([70])}\n"
     ]
    }
   ],
   "source": [
    "disjoint_hypergraph = create_disjoint_hypergraph(model_example_out)\n",
    "print(disjoint_hypergraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "995e69c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule\n",
    "from torchmetrics import Metric\n",
    "import yaml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4973a056",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticlePurity(Metric):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.add_state(\"total_purity\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"num_events\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, disjoint_hypergraph: dict, labels: Tensor):\n",
    "        \"\"\"\n",
    "        Update the metric state with a new disjoint hypergraph and corresponding labels.\n",
    "\n",
    "        Args:\n",
    "            disjoint_hypergraph (dict): A dictionary where keys are hyperedge indices and values are tensors of node indices.\n",
    "            labels (Tensor): A tensor of shape (num_nodes,) containing the labels for each node.\n",
    "        \"\"\"\n",
    "        event_purity = 0.0\n",
    "        num_particles = len( torch.unique(labels) )\n",
    "\n",
    "        for _, nodes in disjoint_hypergraph.items():\n",
    "            node_labels = labels[nodes]\n",
    "            most_common_label = torch.mode(node_labels).values\n",
    "            intersection = (node_labels == most_common_label).sum().item()\n",
    "\n",
    "            num_particles_with_most_common_label = (labels == most_common_label).sum().item()\n",
    "            \n",
    "            if intersection >= 0.5 * len(node_labels) and intersection >= 0.5 * num_particles_with_most_common_label:\n",
    "                event_purity += intersection / num_particles_with_most_common_label\n",
    "\n",
    "        self.total_purity += event_purity / num_particles\n",
    "        self.num_events += 1\n",
    "\n",
    "    def compute(self):\n",
    "        \"\"\"\n",
    "        Compute the average particle purity over all events.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The average particle purity.\n",
    "        \"\"\"\n",
    "        return self.total_purity / self.num_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4bfbde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrackPurity(Metric):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.add_state(\"total_purity\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"num_events\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, disjoint_hypergraph: dict, labels: Tensor):\n",
    "        \"\"\"\n",
    "        Update the metric state with a new disjoint hypergraph and corresponding labels.\n",
    "\n",
    "        Args:\n",
    "            disjoint_hypergraph (dict): A dictionary where keys are hyperedge indices and values are tensors of node indices.\n",
    "            labels (Tensor): A tensor of shape (num_nodes,) containing the labels for each node.\n",
    "        \"\"\"\n",
    "        event_purity = 0.0\n",
    "        num_tracks = len( disjoint_hypergraph )\n",
    "\n",
    "        for _, nodes in disjoint_hypergraph.items():\n",
    "            node_labels = labels[nodes]\n",
    "            most_common_label = torch.mode(node_labels).values\n",
    "            intersection = (node_labels == most_common_label).sum().item()\n",
    "\n",
    "            num_particles_with_most_common_label = (labels == most_common_label).sum().item()\n",
    "            \n",
    "            if intersection >= 0.5 * len(node_labels) and intersection >= 0.5 * num_particles_with_most_common_label:\n",
    "                event_purity += intersection / len(node_labels)\n",
    "\n",
    "        self.total_purity += event_purity / num_tracks\n",
    "        self.num_events += 1\n",
    "\n",
    "    def compute(self):\n",
    "        \"\"\"\n",
    "        Compute the average particle purity over all events.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The average particle purity.\n",
    "        \"\"\"\n",
    "        return self.total_purity / self.num_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8cfb8dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrackML.Embedding.utils import PointCloudData,train_test_split\n",
    "\n",
    "### Define pytorch lightning dataset : \n",
    "class EmbeddingDataset(pl.LightningDataModule): \n",
    "    \n",
    "    # initialize the class : \n",
    "    def __init__(self,hparams)->None: \n",
    "        super().__init__() \n",
    "        self.save_hyperparameters(hparams)\n",
    "        \n",
    "    # def prepare_data(self)->None: \n",
    "        # self.detector = Preprocessing.load_detector_data(self.hparams['detector_path'])\n",
    "        # get the list of event ids from the dataset folder : \n",
    "        # self.eventids = [ code[:-9] for code in os.listdir(self.hparams['dataset_path']) if code.endswith('-hits.csv') ]\n",
    "        # self.dataset = PointCloudData(dataset_path=self.hparams['dataset_path'] , detector_path=self.hparams['detector_path'] , min_nhits=self.hparams['min_hits'] )\n",
    "    \n",
    "    def setup(self,stage=None)->None: \n",
    "        self.dataset = PointCloudData(dataset_path=self.hparams['dataset_path'] , detector_path=self.hparams['detector_path'] , min_nhits=self.hparams['min_hits'] , max_r = self.hparams['max_r'] , drop_fake= self.hparams['drop_fake'] )\n",
    "        self.train_ds , self.val_ds , self.test_ds = train_test_split(\n",
    "            dataset=self.dataset, valid_size=self.hparams['valid_size'], \n",
    "            test_size=self.hparams['test_size'], num_works=self.hparams['num_works']\n",
    "        )\n",
    "        \n",
    "    def train_dataloader(self): \n",
    "        return self.train_ds \n",
    "    def val_dataloader(self): \n",
    "        return self.val_ds \n",
    "    def test_dataloader(self): \n",
    "        return self.test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26bcfc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open( '05-Hypergraph-Model.yml' , 'r' ) as f : \n",
    "    hparams = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6661f2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HGNN_TrackML(LightningModule): \n",
    "    \n",
    "    def __init__(self,hparams): \n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "        \n",
    "        # Metrics (with DDP support)\n",
    "        self.particle_purity = ParticlePurity()\n",
    "        self.track_purity = TrackPurity()\n",
    "        \n",
    "        # Losses\n",
    "        self.loss = EventLossFunction()\n",
    "        \n",
    "        self.model = HGNN(\n",
    "            node_insize = self.hparams['node_insize'],\n",
    "            node_outsize = self.hparams['node_outsize'],\n",
    "            node_features = self.hparams['node_features'],\n",
    "            hpr_edge_outsize = self.hparams['hpr_edge_outsize'],\n",
    "            hpr_edge_features = self.hparams['hpr_edge_features'],\n",
    "            hg_outsize = self.hparams['hg_outsize'],\n",
    "            hg_features = self.hparams['hg_features']\n",
    "        )\n",
    "        \n",
    "        self.last_idx = 0\n",
    "        \n",
    "    def forward(self , x ): \n",
    "        return self.model( x )\n",
    "    \n",
    "    def training_step(self , batch , batch_idx ): \n",
    "        \n",
    "        event_data , labels = batch \n",
    "        event_data.squeeze_(dim=0)\n",
    "        labels.squeeze_(dim=0)\n",
    "        \n",
    "        output = self( event_data )\n",
    "        \n",
    "        loss = self.loss( output , labels )\n",
    "        self.log(\n",
    "            'Loss' , loss, \n",
    "            prog_bar = True , on_step = True , on_epoch = False \n",
    "        )\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.log(\n",
    "                'Memory Allocated' , torch.cuda.memory_allocated()/(1024**3), \n",
    "                prog_bar=True , on_step = True , on_epoch=True, \n",
    "                reduce_fx='max'\n",
    "            )\n",
    "        self.last_idx = batch_idx\n",
    "        return loss \n",
    "    \n",
    "    def on_train_batch_end(self, outputs, batch, batch_idx):\n",
    "        if self.last_idx % 10 == 0 and torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect() \n",
    "    \n",
    "    # common logic for test and validation \n",
    "    def _test_val_common_step_(self , batch , batch_idx ): \n",
    "        \n",
    "        with torch.no_grad(): \n",
    "            event_data , labels = batch \n",
    "            event_data.squeeze_(dim=0)\n",
    "            labels.squeeze_(dim=0)\n",
    "            \n",
    "            output = self( event_data )\n",
    "            \n",
    "            loss = self.loss( output , labels ) \n",
    "            \n",
    "            disjoint_hypergraph = create_disjoint_hypergraph(output)\n",
    "            self.particle_purity.update(disjoint_hypergraph, labels)\n",
    "            self.track_purity.update(disjoint_hypergraph, labels)\n",
    "            \n",
    "            self.last_idx = batch_idx\n",
    "            \n",
    "            return loss \n",
    "    \n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        return self._test_val_common_step_(batch,batch_idx)\n",
    "    \n",
    "    def test_step(self,batch,batch_idx): \n",
    "        return self._test_val_common_step_(batch,batch_idx)\n",
    "    \n",
    "    def on_validation_batch_end(self, outputs, batch, batch_idx, dataloader_idx = 0):\n",
    "        if self.last_idx % 10 == 0 and torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        self.log_dict({\n",
    "            \"Particle Purity\": self.particle_purity.compute(),\n",
    "            \"Track Purity\": self.track_purity.compute(),\n",
    "        }, prog_bar=True)\n",
    "        self.particle_purity.reset()\n",
    "        self.track_purity.reset()\n",
    "\n",
    "        \n",
    "    def on_test_epoch_end(self):\n",
    "        self.log_dict({\n",
    "            \"Particle Purity\": self.particle_purity.compute(),\n",
    "            \"Track Purity\": self.track_purity.compute(),\n",
    "        }, prog_bar=True)\n",
    "        self.particle_purity.reset()\n",
    "        self.track_purity.reset()\n",
    "    \n",
    "    def on_test_batch_end(self, outputs, batch, batch_idx, dataloader_idx = 0):\n",
    "        if self.last_idx % 10 == 0 and torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "    \n",
    "    # set the optimizer  :\n",
    "    def configure_optimizers(self): \n",
    "        return torch.optim.SGD(self.model.parameters(),lr = self.hparams['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb5f4338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4764aa7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/ashmitbathla/Documents/UGP-Local/TrackML/TrackMLVenv/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 2 batch(es). Logging and checkpointing is suppressed.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model = HGNN_TrackML(hparams)\n",
    "ds = EmbeddingDataset(hparams)\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator = \"cpu\", \n",
    "    devices = \"auto\",\n",
    "    enable_checkpointing=False, \n",
    "    # max_epochs=1,\n",
    "    fast_dev_run = 2 , \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6616625",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name            | Type              | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | particle_purity | ParticlePurity    | 0      | train\n",
      "1 | track_purity    | TrackPurity       | 0      | train\n",
      "2 | loss            | EventLossFunction | 0      | train\n",
      "3 | model           | HGNN              | 6.0 K  | train\n",
      "--------------------------------------------------------------\n",
      "6.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.0 K     Total params\n",
      "0.024     Total estimated model params size (MB)\n",
      "33        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/Users/ashmitbathla/Documents/UGP-Local/TrackML/TrackMLVenv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/ashmitbathla/Documents/UGP-Local/TrackML/TrackMLVenv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3437ecd558754f2fb3a5b6855b397cdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    trainer.fit(model , ds )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TrackMLVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
