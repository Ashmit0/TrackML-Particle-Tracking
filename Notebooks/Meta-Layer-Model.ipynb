{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Dataset \n",
    "We build a Pytorch Dataset Object to store and structure the graph data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries : \n",
    "import gc \n",
    "import os \n",
    "import random \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "from typing import Optional, Tuple\n",
    "import seaborn as sns \n",
    "\n",
    "# set random seed : \n",
    "np.random.seed( 41 )\n",
    "random.seed( 41 )\n",
    "\n",
    "import torch\n",
    "import psutil\n",
    "import torch.nn as nn \n",
    "from torch import cdist\n",
    "from torch import Tensor \n",
    "import torch.nn.functional as F \n",
    "import torch.utils.data as data \n",
    "\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import  remove_self_loops , scatter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if CPU is available for training : \n",
    "\n",
    "device = 'gpu'\n",
    "if torch.cuda.is_available(): \n",
    "    device = 'cuda'\n",
    "elif torch.mps.is_available(): \n",
    "    device = 'mps'\n",
    "\n",
    "device = torch.device( device )\n",
    "device "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventData(data.Dataset): \n",
    "    \n",
    "    # initialaize the event dataset \n",
    "    def __init__(self,path:str,device,threshold_dist:float=20)->None:\n",
    "        '''\n",
    "        Inputs : \n",
    "            path: path to the folder where the csv file was contained.  \n",
    "        '''\n",
    "        super(EventData,self).__init__()\n",
    "        self.events = [code[:-9] for code in os.listdir(path) if code.endswith('-hits.csv')]\n",
    "        self.num_events = len(self.events)\n",
    "        self.threshold_dist  = threshold_dist\n",
    "        self.path = path\n",
    "        self.device = device  \n",
    "        \n",
    "    # function returns graph type represntation of the event dataset \n",
    "    def GraphData(self,idx:int) -> Data :\n",
    "        eventid = self.events[idx] \n",
    "        \n",
    "        # read the required csv files : \n",
    "        hits = pd.read_csv(self.path+eventid+'-hits.csv')\n",
    "        truth = pd.read_csv(self.path+eventid+'-truth.csv')\n",
    "        cells = pd.read_csv(self.path+eventid+'-cells.csv')\n",
    "        particles = pd.read_csv( self.path+eventid+'-particles.csv')\n",
    "        particle_ids  = truth.particle_id   \n",
    "        # get charges corrsponding to the hits : \n",
    "        charge = Tensor([ particles.loc[ particles.particle_id.index[ particles.particle_id == ids  ] , 'q' ].tolist()[0] if ids != 0 else 0  for ids in particle_ids ])\n",
    "        del particles , particle_ids \n",
    "        gc.collect()\n",
    "        # find the charges left on the hit ( q = +- 1 )\n",
    "        \n",
    "        \n",
    "        # total number of hits : these form the NODES of our graph. \n",
    "        nhits = hits.shape[0] \n",
    "        # x , y , z spatial featuers of the hits:  \n",
    "        hits_spatial = hits.to_numpy()[: , 1:4 ]\n",
    "        # Add a new feature vector : the number of cells that detect the hit : \n",
    "        node_fets = np.concatenate(\n",
    "            (\n",
    "                hits_spatial ,\n",
    "                cells.hit_id.value_counts().get( hits.hit_id , 0 ).to_numpy().reshape((-1,1))\n",
    "            ), \n",
    "            axis = 1 \n",
    "        )\n",
    "        del cells \n",
    "        gc.collect()\n",
    "        # id's related to the hits \n",
    "        # this will help to initialize the graph structure : \n",
    "        hit_ids = Tensor(hits.hit_id.to_numpy( )).int()\n",
    "        volume_id = Tensor(hits.volume_id.to_numpy( ))\n",
    "        layer_id = Tensor(hits.layer_id.to_numpy( ))\n",
    "        \n",
    "        # get the particle true hit position and momentum, we add this to the node feat matrix : \n",
    "        node_fets = np.concatenate(\n",
    "            (\n",
    "                node_fets , \n",
    "                truth[['tx' , 'ty' , 'tz'  ]].to_numpy() - hits_spatial , \n",
    "                truth[['tpx' , 'tpy' ,'tpz']].to_numpy()\n",
    "            ), \n",
    "            axis = 1 \n",
    "        )\n",
    "        node_fets = Tensor( node_fets )\n",
    "        hits_spatial = Tensor( hits_spatial )\n",
    "        \n",
    "        # here we create edge_index's for the graph skeleton : \n",
    "        batch_size = 10000  # Process 10K nodes at a time\n",
    "        edges = []\n",
    "        \n",
    "        \n",
    "        for i in range(0, nhits, batch_size):\n",
    "            # set distance threshold : \n",
    "            mask = cdist( hits_spatial[i : i + batch_size , : ] , hits_spatial , p = 2 ) < self.threshold_dist\n",
    "            # mask2 ensures hits are either conect to another hit iff the volume_id of the dst > volume if of src or \n",
    "            # layer id of dst > layer id of src in case they have the same volume id \n",
    "            mask2 = volume_id.unsqueeze(0) -  volume_id[i:i+batch_size].unsqueeze(1) >= 0  \n",
    "            mask2 = mask2 | ((volume_id.unsqueeze(0) == volume_id[i:i+batch_size].unsqueeze(1) ) & (layer_id.unsqueeze(0) - layer_id[i:i+batch_size].unsqueeze(1) >= 0 ))\n",
    "            mask = mask & ( charge.unsqueeze(0) - charge[i:i+batch_size].unsqueeze(1) == 0 )\n",
    "            # ensure both conditions are satisfied \n",
    "            mask = mask & mask2 \n",
    "            del mask2 \n",
    "            gc.collect()\n",
    "            src, dst = torch.where(mask)  # Get valid edges\n",
    "            del mask \n",
    "            gc.collect()\n",
    "            edges.append(torch.stack([src + i, dst]))  # Offset indices\n",
    "            del src , dst \n",
    "            gc.collect()\n",
    "\n",
    "        del volume_id , layer_id , hits_spatial \n",
    "        gc.collect()\n",
    "        edge_index = torch.cat(edges, dim=1)\n",
    "        del edges \n",
    "        gc.collect()\n",
    "        # remove self loops from the edge_index thus generated : \n",
    "        edge_index, _ = remove_self_loops(edge_index)\n",
    "        row , col = edge_index \n",
    "        \n",
    "        # number of edges : \n",
    "        num_edges = edge_index.shape[1]\n",
    "        \n",
    "        # create edge labels and edge attributes : \n",
    "        # Lables : \n",
    "            # label == 0 if the two nodes are not part of a traj \n",
    "            # label == 1 otherwise \n",
    "        edge_labels = ((truth.particle_id.to_numpy()[row] == truth.particle_id.to_numpy()[col]) & ( truth.particle_id.to_numpy()[row] != 0 ))\n",
    "        edge_labels = Tensor( edge_labels ).float()\n",
    "        \n",
    "        # Attributes : \n",
    "            # Angle: between the momentum vector of the particle and the displacement vector between the hits. \n",
    "            # Distance: euclidean distance between the two hits. \n",
    "        pVector = Tensor( truth[['tpx' , 'tpy' ,'tpz']].to_numpy()[row] )\n",
    "        pVector = pVector/torch.linalg.norm( pVector , ord = 2 , dim = 1 , keepdim= True )\n",
    "        disp = Tensor( hits[['x','y','z']].to_numpy()[row] -  hits[['x','y','z']].to_numpy()[col] )\n",
    "        del row , col \n",
    "        gc.collect()\n",
    "        dist = torch.linalg.norm( disp , ord = 2 , dim = 1 , keepdim=True )\n",
    "        angle = torch.sum( pVector*(disp/dist) , dim = 1 , keepdim=True )\n",
    "        angle[torch.isnan(angle)] = 0.\n",
    "        del pVector , disp  \n",
    "        gc.collect()\n",
    "        edge_attr = torch.cat([angle , dist] , dim = 1 )\n",
    "        del angle , dist , hits , truth \n",
    "        gc.collect()\n",
    "        \n",
    "        # define graph data : \n",
    "        graph_data = Data(\n",
    "            x = node_fets , \n",
    "            edge_index=edge_index , \n",
    "            edge_attr = edge_attr , \n",
    "            label = edge_labels.unsqueeze(1) , \n",
    "            num_nodes = nhits , \n",
    "            num_edges = num_edges ,\n",
    "            hit_ids = hit_ids \n",
    "        )\n",
    "        \n",
    "        return graph_data \n",
    "    \n",
    "    def __len__(self)->int: \n",
    "        return self.num_events \n",
    "    \n",
    "    def __getitem__(self,index:int)->Data:\n",
    "        return self.GraphData(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test event data code : \n",
    "dataset = EventData(path='../data/train_100_events/',device=device)\n",
    "size = len( dataset )\n",
    "size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnum = np.random.choice(np.arange(size))\n",
    "# random_event = dataset[rnum]\n",
    "# random_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a function that builds a MultiLayerPercerptron : \n",
    "\n",
    "def buildMLP( insize:int, outsize:int, features:list, add_bnorm:bool = False, add_activation=None): \n",
    "    layers = [] \n",
    "    layers.append(nn.Linear( insize , features[0]))\n",
    "    layers.append( nn.ReLU() )\n",
    "    for i in range( 1 , len( features ) ): \n",
    "        if add_bnorm: \n",
    "            layers.append( nn.BatchNorm1d( features[i-1]) )\n",
    "        layers.append( nn.Linear( features[i-1] , features[i] ) )\n",
    "        layers.append( nn.ReLU() )\n",
    "    layers.append(nn.Linear(features[-1],outsize))\n",
    "    if add_activation != None: \n",
    "        layers.append( add_activation )\n",
    "    return nn.Sequential(*layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the dataloder for the loading the graph data in batches of 5 \n",
    "dataset_loder = DataLoader(dataset=dataset, batch_size=2, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = next(iter(dataset))\n",
    "batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(batch_data.num_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the meta layer class \n",
    "class MetaLayer( torch.nn.Module ): \n",
    "    def __init__( self,\n",
    "                 edge_model: Optional[torch.nn.Module] = None ,\n",
    "                 node_model: Optional[torch.nn.Module] = None ,\n",
    "                 global_model: Optional[torch.nn.Module] = None ):\n",
    "        \n",
    "        super(MetaLayer, self).__init__()\n",
    "        self.edge_model = edge_model\n",
    "        self.node_model = node_model\n",
    "        self.global_model = global_model\n",
    "    \n",
    "        # self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Resets all learnable parameters of the module.\"\"\"\n",
    "        for item in [self.node_model, self.edge_model, self.global_model]:\n",
    "            if hasattr(item, 'reset_parameters'):\n",
    "                item.reset_parameters()\n",
    "                \n",
    "    def forward(\n",
    "        self,\n",
    "        x:Tensor, edge_index:Tensor, \n",
    "        edge_attr: Optional[Tensor] = None, \n",
    "        u : Optional[Tensor] = None, \n",
    "        batch : Optional[Tensor] = None\n",
    "    ) -> Tuple[ Tensor , Optional[Tensor] , Optional[Tensor] ] : \n",
    "        \n",
    "        row , col = edge_index[0] , edge_index[1] \n",
    "        \n",
    "        y =  batch if batch is None else batch[row] \n",
    "        # print( x.shape )\n",
    "        # Edge level step \n",
    "        if self.edge_model is not None: \n",
    "            edge_attr = self.edge_model( x[row] , x[col] , edge_attr, u ,   y  ) \n",
    "        # Node level Step \n",
    "        if self.node_model is not None: \n",
    "            x = self.node_model(x,edge_index,edge_attr,u,batch) \n",
    "        # Graph Level Step \n",
    "        if self.global_model is not None: \n",
    "            u = self.global_model(x,edge_index,edge_attr,u,batch)  \n",
    "        \n",
    "        return x , edge_attr , u \n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}(\\n'\n",
    "                f'  edge_model={self.edge_model},\\n'\n",
    "                f'  node_model={self.node_model},\\n'\n",
    "                f'  global_model={self.global_model}\\n'\n",
    "                f')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the edge network : \n",
    "\n",
    "class EdgeNet(nn.Module): \n",
    "    def __init__(\n",
    "        self, in_edge:int, \n",
    "        out_edge:int, node_dim:int,\n",
    "        features:list  , global_dim:Optional[int] = None \n",
    "    ):\n",
    "        super( EdgeNet , self ).__init__()\n",
    "        if global_dim is None : \n",
    "            global_dim = 0 \n",
    "        self.edge_mlp =  buildMLP(\n",
    "            insize= 2*node_dim + global_dim +  in_edge , \n",
    "            outsize= out_edge , features= features\n",
    "        )\n",
    "    \n",
    "    def forward(\n",
    "        self,src:Tensor,dst:Tensor, \n",
    "        edge_attr:Tensor, \n",
    "        u:Optional[Tensor]=None, \n",
    "        edge_batch:Optional[Tensor]=None\n",
    "    ): \n",
    "        if (u is not None) and (edge_batch is None) : \n",
    "            raise ValueError('Must Pass edge_batch if global data is present' )\n",
    "        \n",
    "        out = torch.cat([src,dst,edge_attr],dim=1)\n",
    "        \n",
    "        if u is not None : \n",
    "            out = torch.cat([out,u[edge_batch]],dim=1)\n",
    "            \n",
    "        return self.edge_mlp(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us check the model's vaidity : \n",
    "\n",
    "# number of node and edge featuers : \n",
    "num_node_feat = batch_data.x.shape[1] \n",
    "out_node_feat = 2*num_node_feat \n",
    "num_edge_feat = batch_data.edge_attr.shape[1]\n",
    "out_edge_feat = 2*num_edge_feat \n",
    "\n",
    "# define the edge_network : \n",
    "edge_network =  EdgeNet( in_edge= num_edge_feat ,out_edge=out_edge_feat, node_dim=num_node_feat , features=[3 , 4 , 2 ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "srcidx shape == torch.Size([1995932])\n",
      "Shape of Edge Attributes before update torch.Size([1995932, 2])\n",
      "Shape of Edge Attributes after update torch.Size([1995932, 4])\n"
     ]
    }
   ],
   "source": [
    "# get the source and destination indesis : \n",
    "srcidx , dstidx = batch_data.edge_index \n",
    "print( 'srcidx shape ==' ,  srcidx.shape )\n",
    "\n",
    "src , dst = batch_data.x[srcidx] , batch_data.x[dstidx]\n",
    "\n",
    "\n",
    "updated_edge = edge_network( src , dst , batch_data.edge_attr )\n",
    "print('Shape of Edge Attributes before update' , batch_data.edge_attr.shape )\n",
    "print('Shape of Edge Attributes after update' , updated_edge.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node update block : \n",
    "\n",
    "class NodeNet( torch.nn.Module ): \n",
    "    def __init__(\n",
    "        self, innode:int, outnode:int, \n",
    "        inedge:Optional[int] = 0 , \n",
    "        inglobal:Optional[int] = 0 ,  \n",
    "        features:Optional[list] = 0 \n",
    "    ): \n",
    "        \n",
    "        super( NodeNet , self ).__init__()\n",
    "        self.node_mlp = buildMLP(\n",
    "            insize=innode+inedge+inglobal, \n",
    "            outsize=outnode , \n",
    "            features=features\n",
    "        )\n",
    "        \n",
    "    def forward(\n",
    "        self , x:Tensor , \n",
    "        edge_index:Tensor, \n",
    "        edge_attr:Optional[Tensor]=None, \n",
    "        u:Optional[Tensor]=None, \n",
    "        batch:Optional[Tensor]=None \n",
    "    ): \n",
    "        if u is not None and batch is None : \n",
    "            raise ValueError('Must Pass edge_batch if global data is present' )\n",
    "        \n",
    "        _ , col = edge_index \n",
    "        \n",
    "        out = x \n",
    "        \n",
    "        if edge_attr is not None : \n",
    "            y = scatter(\n",
    "                edge_attr , col , dim = 0 , \n",
    "                dim_size=x.size(0) , reduce='mean'\n",
    "            )\n",
    "            out = torch.cat( [ out , y  ] , dim = 1 )\n",
    "            del y \n",
    "        \n",
    "        if u is not None : \n",
    "            out = torch.cat( out , u[batch] , dim = 1 )\n",
    "             \n",
    "        return self.node_mlp( out )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node featuers shape before update : torch.Size([115584, 10])\n",
      "Node featuers after update: torch.Size([115584, 20])\n"
     ]
    }
   ],
   "source": [
    "# here we test our node update part of the model : \n",
    "\n",
    "# define the node network : \n",
    "node_network = NodeNet(\n",
    "    inedge = out_edge_feat, \n",
    "    innode = num_node_feat , \n",
    "    outnode = out_node_feat, inglobal=0, \n",
    "    features=[12 , 14 , 17 ] \n",
    ")\n",
    "\n",
    "# get the updated node features : \n",
    "updated_node = node_network(\n",
    "    x = batch_data.x , \n",
    "    edge_index = batch_data.edge_index , \n",
    "    edge_attr = updated_edge \n",
    ")\n",
    "\n",
    "print('Node featuers shape before update :' , batch_data.x.shape ) \n",
    "print('Node featuers after update:' , updated_node.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Update Block : \n",
    "\n",
    "class GlobalNet( torch.nn.Module ): \n",
    "    def __init__(\n",
    "        self, \n",
    "        inglobal:int, outglobal:int, \n",
    "        features:list, innode:int , \n",
    "        inedge:Optional[int]=0 \n",
    "    ): \n",
    "        \n",
    "        super( GlobalNet , self ).__init__() \n",
    "        self.global_mlp = buildMLP(\n",
    "            insize=inedge+innode+inglobal, \n",
    "            outsize=outglobal, \n",
    "            features=features\n",
    "        )\n",
    "        \n",
    "    def forward(\n",
    "        self, x:Tensor, \n",
    "        edge_index:Tensor,\n",
    "        u:Tensor, batch:Tensor , \n",
    "        edge_attr:Optional[Tensor]=None\n",
    "    ): \n",
    "        \n",
    "        src_idx , _ = edge_index \n",
    "        \n",
    "        out = torch.cat([u, scatter(x,batch,dim=0,reduce='mean')] , dim = 1 )\n",
    "        if edge_attr is not None : \n",
    "            out = torch.cat([u,scatter(edge_attr,batch[src_idx],dim=0,reduce='mean')],dim=1)\n",
    "        \n",
    "        return self.global_mlp( out )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post meta layer node featuers shape torch.Size([115584, 20])\n",
      "Post meta layer edge featuers shape torch.Size([1995932, 4])\n"
     ]
    }
   ],
   "source": [
    "# Now we will see a full meta layer in action : \n",
    "\n",
    "# note for our purpose we only need node and edge networks \n",
    "# and do not need a global update block :\n",
    "example_meta_layer = MetaLayer(\n",
    "    edge_model=edge_network , \n",
    "    node_model=node_network\n",
    ")\n",
    "\n",
    "post_meta_layer_nodes , post_meta_layer_edges , _ = example_meta_layer(\n",
    "    x = batch_data.x , \n",
    "    edge_index = batch_data.edge_index , \n",
    "    edge_attr = batch_data.edge_attr \n",
    ")\n",
    "\n",
    "print( 'Post meta layer node featuers shape' , post_meta_layer_nodes.shape )\n",
    "print( 'Post meta layer edge featuers shape' , post_meta_layer_edges.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we define the full GNN Model using metalayers: \n",
    "\n",
    "class GNN_MetaLayer_Model(torch.nn.Module): \n",
    "    \n",
    "    def __init__(\n",
    "        self , nmeta_layers:int, \n",
    "        node_feats:list, \n",
    "        inter_node_feats:list, \n",
    "        edge_feats:Optional[list]=None, \n",
    "        inter_edge_feats:Optional[list]=None, \n",
    "        global_feats:Optional[list]=None, \n",
    "        inter_global_feats:Optional[list]=None \n",
    "    ):\n",
    "        super(GNN_MetaLayer_Model , self ).__init__()\n",
    "        \n",
    "        self.meta_layers = nn.ModuleList([])\n",
    "        \n",
    "        if inter_node_feats == None : \n",
    "            raise ValueError('Inter Node feats must also be supplied along with node_feats')\n",
    "        if len( node_feats ) != nmeta_layers + 1 : \n",
    "            raise ValueError('The length of \\'node_feats\\' must be equal to nmeta_layers + 1 ' )\n",
    "        if len( inter_node_feats ) != nmeta_layers : \n",
    "            raise ValueError('The length of \\'inter_node_feats\\' must be equal to nmeta_layers ' )\n",
    "        \n",
    "        if edge_feats != None : \n",
    "            if inter_edge_feats == None : \n",
    "                raise ValueError('Inter edge feats must also be supplied along with edge_feats')\n",
    "            if len( edge_feats ) != nmeta_layers + 1 : \n",
    "                raise ValueError('The length of \\'edge_feats\\' must be equal to nmeta_layers + 1 ' )\n",
    "            if len( inter_edge_feats ) != nmeta_layers  : \n",
    "                raise ValueError('The length of \\'inter_edge_feats\\' must be equal to nmeta_layers ' )\n",
    "            \n",
    "        if global_feats != None : \n",
    "            if inter_global_feats == None : \n",
    "                raise ValueError('Inter global feats must also be supplied along with global_feats')\n",
    "            if len( global_feats ) != nmeta_layers + 1 : \n",
    "                raise ValueError('The length of \\'global_feats\\' must be equal to nmeta_layers + 1 ' )\n",
    "            if len( inter_global_feats ) != nmeta_layers : \n",
    "                raise ValueError('The length of \\'inter_global_feats\\' must be equal to nmeta_layers ' )\n",
    "        \n",
    "        if (node_feats is None ) and (edge_feats is None ) and (global_feats is None ) : \n",
    "            raise ValueError('All Type of Netorks are Null')\n",
    "        \n",
    "        self.nmeta = nmeta_layers\n",
    "        \n",
    "        edge_part , node_part , global_part = None , None , None \n",
    "        for i in range( nmeta_layers ): \n",
    "            \n",
    "            if global_feats is not None : \n",
    "                current_global_feat = global_feats[i]\n",
    "            else : \n",
    "                current_global_feat = 0 \n",
    "    \n",
    "            if edge_feats is not None : \n",
    "                edge_part = EdgeNet(\n",
    "                    in_edge=edge_feats[i],\n",
    "                    out_edge=edge_feats[i+1] , \n",
    "                    node_dim=node_feats[i] , \n",
    "                    global_dim=current_global_feat, \n",
    "                    features=inter_node_feats[i]\n",
    "                )\n",
    "                current_edge_feat = edge_feats[i+1]\n",
    "            else : \n",
    "                current_edge_feat = 0\n",
    "                \n",
    "            if node_feats is not None : \n",
    "                node_part = NodeNet(\n",
    "                    innode=node_feats[i] , \n",
    "                    outnode=node_feats[i+1],\n",
    "                    features=inter_node_feats[i],\n",
    "                    inedge=current_edge_feat,\n",
    "                    inglobal=current_global_feat \n",
    "                )\n",
    "            if global_feats is not None : \n",
    "                global_part = GlobalNet(\n",
    "                    inedge=current_edge_feat,\n",
    "                    innode=node_feats[i+1],\n",
    "                    inglobal=global_feats[i] , \n",
    "                    outglobal=global_feats[i+1] , \n",
    "                    features=inter_global_feats[i]\n",
    "                )\n",
    "            self.meta_layers.append(\n",
    "                MetaLayer(\n",
    "                    edge_model=edge_part,\n",
    "                    node_model=node_part,\n",
    "                    global_model=global_part\n",
    "                )\n",
    "            )\n",
    "            \n",
    "    def forward( self , data ):\n",
    "        x , edge_attr , global_data = data.x , None , None \n",
    "        \n",
    "        if 'edge_attr' in data : \n",
    "            edge_attr = data.edge_attr  \n",
    "            \n",
    "        if 'global_data' in data : \n",
    "            global_data = data.global_data \n",
    "        \n",
    "        for i in range(self.nmeta) : \n",
    "            x , edge_attr , global_data = self.meta_layers[i](\n",
    "                x = x , edge_index = data.edge_index , \n",
    "                edge_attr = edge_attr , u = global_data , batch = data.batch     \n",
    "            )\n",
    "            \n",
    "        # return final updated  featuers ! \n",
    "        return x , edge_attr , global_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([115584, 10]) torch.Size([115584, 1])\n",
      "torch.Size([1995932, 2]) torch.Size([1995932, 1])\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        ...,\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "# now we test the model code : \n",
    "\n",
    "model = GNN_MetaLayer_Model(\n",
    "    nmeta_layers=2,\n",
    "    node_feats=[num_node_feat,4,1] , \n",
    "    inter_node_feats=[[8,6],[3,2]] , \n",
    "    edge_feats=[num_edge_feat,7,1] , \n",
    "    inter_edge_feats=[[3,5],[5,3]] , \n",
    ")\n",
    "\n",
    "\n",
    "updated_node , updated_edge , _ = model(batch_data)\n",
    "print( batch_data.x.shape , updated_node.shape )\n",
    "print( batch_data.edge_attr.shape , updated_edge.shape )\n",
    "print( batch_data.label )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[115584, 10], edge_index=[2, 1995932], edge_attr=[1995932, 2], label=[1995932, 1], num_nodes=115584, num_edges=1995932, hit_ids=[115584])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we write train and test functions : \n",
    "\n",
    "def train(model, train_loader, optimizer)->float:\n",
    "    \n",
    "    train_loss_ep , data_pts = 0. , 0 \n",
    "    \n",
    "    model.train()\n",
    "    for _ , data in enumerate(train_loader):\n",
    "        # data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        _ , edge_attr , _  = model(data)\n",
    "        loss = F.binary_cross_entropy_with_logits(edge_attr, data.label , reduction='sum' )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print( loss )\n",
    "        train_loss_ep += loss.item() \n",
    "        print( train_loss_ep )\n",
    "        data_pts += data.num_edges \n",
    "        \n",
    "    return train_loss_ep/data_pts\n",
    "\n",
    "\n",
    "def test(model, test_loader)->float:\n",
    "    \n",
    "    test_loss_ep  , data_pts = 0. , 0 \n",
    "    \n",
    "    model.eval()\n",
    "    for _ , data in enumerate(test_loader):\n",
    "        # data, target = data.to(device), target.to(device)\n",
    "         \n",
    "        _ , edge_attr , _  = model(data)\n",
    "        loss = F.binary_cross_entropy_with_logits(edge_attr, data.label , reduction='sum' )\n",
    "        \n",
    "        test_loss_ep += loss.item() \n",
    "        data_pts += data.num_edges \n",
    "        \n",
    "    return test_loss_ep/data_pts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 2\n",
    "# percentage of training set to use as validation\n",
    "train_size, valid_size = 0.1 , 0.1 \n",
    "# # convert data to torch.FloatTensor\n",
    "# transform = transforms.ToTensor()\n",
    "# choose the training and testing datasets\n",
    "# obtain training indices that will be used for validation\n",
    "dataset = EventData(path='../data/train_100_events/',device=device)\n",
    "num_train = len(dataset)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "train_split = int(np.floor(train_size * num_train))\n",
    "valid_split = int(np.floor(valid_size * num_train))\n",
    "\n",
    "print(train_split, valid_split)\n",
    "train_index, valid_index, test_index = indices[0:train_split], indices[train_split:train_split + valid_split], indices[train_split + valid_split:]\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_index)\n",
    "valid_sampler = SubsetRandomSampler(valid_index)\n",
    "test_sampler = SubsetRandomSampler(test_index)\n",
    "# prepare data loaders\n",
    "train_loader = DataLoader(dataset=dataset, batch_size = batch_size, \n",
    "                                           num_workers = num_workers, sampler = train_sampler  )\n",
    "valid_loader = DataLoader(dataset=dataset, batch_size = batch_size,\n",
    "                                          num_workers = num_workers,  sampler = valid_sampler  )\n",
    "test_loader = DataLoader(dataset=dataset, batch_size = batch_size,\n",
    "                                          num_workers = num_workers,  sampler = test_sampler )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Define the optimizer --- #\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1720788.7500, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "1720788.75\n",
      "tensor(nan, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "nan\n",
      "tensor(nan, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "nan\n",
      "tensor(nan, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x1075077f0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ashmitbathla/Documents/UGP/TrackML/VirtualEnv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 5\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.inf  # set initial \"min\" to infinity\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor losses\n",
    "    \n",
    "    train_loss = train(model, train_loader, optimizer)\n",
    "    \n",
    "    valid_loss = test(model, valid_loader)\n",
    "    \n",
    "    # print training/validation statistics \n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss\n",
    "        ))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), '../data/models_/event_trained_model.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to Tensor.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEpoch: \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43mTraining Loss: \u001b[39;49m\u001b[38;5;132;43;01m{:.6f}\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43mValidation Loss: \u001b[39;49m\u001b[38;5;132;43;01m{:.6f}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalid_loss\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Documents/UGP/TrackML/VirtualEnv/lib/python3.10/site-packages/torch/_tensor.py:1098\u001b[0m, in \u001b[0;36mTensor.__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m Tensor:\n\u001b[1;32m   1097\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(format_spec)\n\u001b[0;32m-> 1098\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__format__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_spec\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported format string passed to Tensor.__format__"
     ]
    }
   ],
   "source": [
    "print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([47.9358, 48.7042])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VirtualEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
