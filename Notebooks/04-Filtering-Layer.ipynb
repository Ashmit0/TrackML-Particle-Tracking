{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering MLP Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version 2.6.0\n",
      "PyG version 2.6.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import ortho_group\n",
    "\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.linalg import norm \n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, ReLU, BatchNorm1d, Module, Sequential\n",
    "from torch import Tensor\n",
    "\n",
    "# torch.set_default_dtype(torch.float64)\n",
    "\n",
    "from torch_geometric.typing import (\n",
    "    Adj,\n",
    "    OptPairTensor,\n",
    "    OptTensor,\n",
    "    Size,\n",
    "    SparseTensor,\n",
    "    torch_sparse,\n",
    ")\n",
    "\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import Batch\n",
    "import torch_geometric.transforms as T\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch_geometric.utils import remove_self_loops, to_dense_adj, dense_to_sparse, is_undirected , to_undirected, contains_self_loops , to_networkx\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import MessagePassing, global_mean_pool, knn_graph\n",
    "from torch_geometric.datasets import QM9\n",
    "# from torch_scatter import scatter\n",
    "# from torch_cluster import knn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "# import uproot\n",
    "import vector\n",
    "vector.register_awkward()\n",
    "import awkward as ak\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "print(\"PyTorch version {}\".format(torch.__version__))\n",
    "print(\"PyG version {}\".format(torch_geometric.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrackML.Models.utils import buildMLP\n",
    "from TrackML.Embedding.dataset import PointCloudData\n",
    "from TrackML.Embedding.base import EmbeddingBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 5 \n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostEmbeddingGraph(data.Dataset): \n",
    "    \n",
    "    # initialize the dataset class : \n",
    "    def __init__(\n",
    "        self,dataset_path:str,\n",
    "        detector_path:str,embd_model_path:str,\n",
    "        min_nhits=3,margin:float=0.1, \n",
    "        max_num_neighbors:int=64\n",
    "        )->None:\n",
    "        '''\n",
    "        dataset_path : path to the dataset with the events. \n",
    "        detector_path : path to the detector.csv file \n",
    "        min_nhits : upper bound on the number of  hits to keep. \n",
    "        margin : margin around which to create the radius graph \n",
    "        embd_model_path : path to the saved embedding model. \n",
    "        returns : For each event, returs the PyG graph data that \n",
    "            is constructed post embedding. \n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        self.margin = margin \n",
    "        self.max_num_neighbors = max_num_neighbors\n",
    "        \n",
    "        # get the point cloud dataset: \n",
    "        self.dataset = PointCloudData(\n",
    "            dataset_path=dataset_path, \n",
    "            detector_path=detector_path, \n",
    "            min_nhits=min_nhits\n",
    "        )\n",
    "        \n",
    "        # save the embedding model \n",
    "        self.embd_model = EmbeddingBase.load_from_checkpoint(embd_model_path)\n",
    "        self.embd_model.eval()\n",
    "        \n",
    "        \n",
    "    def __len__(self)->int: \n",
    "        return len( self.dataset )\n",
    "    \n",
    "    def __getitem__(self, index)->Data:\n",
    "        \n",
    "        # get the raw node featuers and labels \n",
    "        node_feats , labels = self.dataset[index]\n",
    "        # get the latent space feat of the nodes : \n",
    "        latent_feats = self.embd_model(node_feats)\n",
    "        \n",
    "        # get the radius graph edge index : \n",
    "        radius_graph_edge_index = torch_geometric.nn.pool.radius_graph(\n",
    "            latent_feats, \n",
    "            r = self.margin , \n",
    "            loop = False , \n",
    "            max_num_neighbors=self.max_num_neighbors\n",
    "        )\n",
    "        \n",
    "        # get the edges labels, 0 if they belong to the same track, 1 otherwise : \n",
    "        row , col = radius_graph_edge_index \n",
    "        edge_attr = (labels[row] != labels[col]).float().unsqueeze_(dim=1)\n",
    "        \n",
    "        # edge_purity ; \n",
    "        edge_purity = 1 - torch.sum( edge_attr )/radius_graph_edge_index.shape[1]\n",
    "        \n",
    "        # create the graph data structure : \n",
    "        graph_data = Data(\n",
    "            x = node_feats, \n",
    "            edge_index=radius_graph_edge_index, \n",
    "            edge_attr=edge_attr, \n",
    "            y = labels , \n",
    "            edge_purity = edge_purity\n",
    "        )\n",
    "        \n",
    "        return graph_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../data/train_100_events/'\n",
    "detector_path= '../data/detectors.csv'\n",
    "embd_model_path =  '../data/models/Embedding-Model-v7.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[96872, 15], edge_index=[2, 6199808], edge_attr=[6199808, 1], y=[96872], edge_purity=0.0737377405166626)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_dataset = PostEmbeddingGraph(\n",
    "    dataset_path , \n",
    "    detector_path , \n",
    "    embd_model_path\n",
    ")\n",
    "\n",
    "filter_data_instance = filter_dataset[10]\n",
    "filter_data_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0737)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_data_instance.edge_purity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx\n",
    "from networkx import connected_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "592"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_disconnected_components(data:Data):\n",
    "    # Convert PyG graph to NetworkX graph\n",
    "    G = to_networkx(data, to_undirected=True)\n",
    "    \n",
    "    # Get connected components\n",
    "    component_list = list(connected_components(G))\n",
    "    \n",
    "    # Convert node indices back to PyTorch tensors\n",
    "    components = [torch.tensor(list(component)) for component in component_list]\n",
    "    \n",
    "    return components\n",
    "\n",
    "example_components = get_disconnected_components(filter_data_instance)\n",
    "len(example_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_reconstruction_metrics( graph_data:Data  )->tuple: \n",
    "    '''\n",
    "        data : graph dataset to get the metrics for : \n",
    "        labels : particle_ids for each of the nodes (hits)\n",
    "        returns : the trajecteory and particle purity of the graph dataset. \n",
    "    '''\n",
    "    \n",
    "    labels = graph_data.y \n",
    "    \n",
    "    # get the disconnected compoenents (reconstructed tracks) \n",
    "    # form the graph steructure : \n",
    "    disconnected_components = get_disconnected_components(graph_data)\n",
    "    \n",
    "    #  partices with the max occourence for each of the disconnected components : \n",
    "    max_track_particle = torch.tensor([ torch.mode(labels[component])[0] for component in disconnected_components])\n",
    "    # frequency of the particle with the most occourence for each of the disconnected component : \n",
    "    max_track_particle_freq = torch.tensor([ torch.sum(labels[track] == particle) for particle,track in zip(max_track_particle,disconnected_components)])\n",
    "    # get the number of hits that belong to each reconstructed track : \n",
    "    num_hits_tracks = torch.tensor( [component.shape[0] for component in disconnected_components] )\n",
    "    # total number of true hits left by the underlying max_track_particle : \n",
    "    max_track_particle_num_true_hits = torch.tensor([torch.sum(labels==particle) for particle in max_track_particle])\n",
    "    \n",
    "    # 2. Get the Track Purity : \n",
    "    track_purity  = torch.mean(max_track_particle_freq/num_hits_tracks)\n",
    "    # 3. Get Particle Purity : \n",
    "    particle_purity = torch.mean(max_track_particle_freq/max_track_particle_num_true_hits)\n",
    "    \n",
    "    return  track_purity , particle_purity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2034), tensor(0.0157))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_reconstruction_metrics( filter_data_instance )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilteringModel(nn.Module): \n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        in_features:int, \n",
    "        hidden_features:list, \n",
    "    )->None:\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # the acctual in featuers would be twice the the \n",
    "        # number of acctual node features since a edge feature \n",
    "        # is a concatination of two node featuers \n",
    "        \n",
    "        in_features *= 2 \n",
    "        # define the MLP layer : \n",
    "        self.MLP = buildMLP(\n",
    "            insize=in_features, \n",
    "            outsize=1, \n",
    "            features=hidden_features,\n",
    "            add_bnorm=True,\n",
    "            add_activation=None\n",
    "        )\n",
    "        \n",
    "    def forward(self,data:Data)->Tensor: \n",
    "        row , col = data.edge_index \n",
    "        edge_feats = torch.cat((data.x[row] , data.x[col]) , dim  = 1 )\n",
    "        return self.MLP( edge_feats )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6199808, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_model_instance = FilteringModel(in_features=15,hidden_features= [20,10,5])\n",
    "filter_model_out = filter_model_instance(filter_data_instance)\n",
    "filter_model_out.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(\n",
    "    dataset:PostEmbeddingGraph,\n",
    "    valid_size:float,\n",
    "    test_size:float,\n",
    "    num_works:int=4\n",
    "):\n",
    "    '''\n",
    "    valid_size : amount of data to reserve for validation (normalized to 1 )\n",
    "    test_size : amount of data to reserve for testing (normalized to 1 )\n",
    "    Returns : train/validation/test data loders. \n",
    "    '''\n",
    "    \n",
    "    train_size=1-test_size-valid_size\n",
    "    \n",
    "    if not ( (train_size <= 1.) & (valid_size <= 1.) & (test_size <= 1. )) : \n",
    "        raise ValueError('Improper valid/train size encountered.')\n",
    "    \n",
    "    # total number of events : \n",
    "    num_events = len(dataset)\n",
    "    \n",
    "    # get shuffeled indices \n",
    "    indices = list(range(num_events))\n",
    "    np.random.shuffle(indices)\n",
    "    train_split = int(np.floor(train_size * num_events))\n",
    "    valid_split = int(np.floor(valid_size * num_events))\n",
    "    \n",
    "    train_index, valid_index, test_index = indices[0:train_split], indices[train_split:train_split + valid_split], indices[train_split + valid_split:]\n",
    "    \n",
    "    # define samplers for obtaining training and validation batches\n",
    "    train_sampler = SubsetRandomSampler(train_index)\n",
    "    valid_sampler = SubsetRandomSampler(valid_index)\n",
    "    test_sampler = SubsetRandomSampler(test_index)\n",
    "    \n",
    "    # define data loaders : \n",
    "    train_loder = DataLoader(\n",
    "        dataset=dataset, \n",
    "        batch_size=1, \n",
    "        num_workers=num_works, \n",
    "        sampler = train_sampler,\n",
    "        persistent_workers=True if num_works > 0 else False\n",
    "    )\n",
    "    \n",
    "    valid_loder = DataLoader(\n",
    "        dataset=dataset, \n",
    "        batch_size=1, \n",
    "        num_workers=num_works, \n",
    "        sampler = valid_sampler,\n",
    "        persistent_workers=True if num_works > 0 else False\n",
    "    )\n",
    "    \n",
    "    test_loder = DataLoader(\n",
    "        dataset=dataset, \n",
    "        batch_size=1, \n",
    "        num_workers=num_works, \n",
    "        sampler = test_sampler,\n",
    "        persistent_workers=True if num_works > 0 else False\n",
    "    )\n",
    "    \n",
    "    # return data loders : \n",
    "    return train_loder,valid_loder,test_loder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loder_instance , val_loder_instance , _   = train_test_split(\n",
    "    filter_dataset , \n",
    "    0.2 , 0.1 , 0 \n",
    ")\n",
    "len( train_loder_instance )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBatch(x=[104514, 15], edge_index=[2, 6688896], edge_attr=[6688896, 1], y=[104514], edge_purity=[1], batch=[104514], ptr=[2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = next(iter(train_loder_instance))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainFiltering(\n",
    "    model:FilteringModel,\n",
    "    train_loder:DataLoader, \n",
    "    lr:float=0.01\n",
    "): \n",
    "    \n",
    "    # initialize optimizer : \n",
    "    optimizer = torch.optim.SGD(model.parameters(),lr = lr)\n",
    "    # initialize train loss\n",
    "    train_loss = 0.0 \n",
    "    # total number of events : \n",
    "    num_events = len(train_loder)\n",
    "    \n",
    "    # loop  over the training dataset \n",
    "    for i,graph_data in tqdm(enumerate(train_loder), bar_format='{l_bar}{bar}| Event {n_fmt}/{total_fmt} [{elapsed}<{remaining}, ' '{rate_fmt}{postfix}]' , total = len(train_loder) , ncols = 75) : \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(graph_data)\n",
    "        \n",
    "        loss_fn = nn.BCEWithLogitsLoss(\n",
    "            reduction='sum' , pos_weight=graph_data.edge_purity/( 1-graph_data.edge_purity)\n",
    "        )\n",
    "        loss = loss_fn(output,graph_data.edge_attr)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        if( i == 2 ) : \n",
    "            break \n",
    "        \n",
    "    return train_loss/num_events\n",
    "\n",
    "\n",
    "def TestEmbedding(\n",
    "    model:PostEmbeddingGraph, \n",
    "    test_loder:DataLoader\n",
    "):\n",
    "    \n",
    "    train_loss = 0.0 \n",
    "    \n",
    "    num_events = len(test_loder)\n",
    "    model.eval()\n",
    "    \n",
    "    # loop  over the training dataset \n",
    "    for i,graph_data in tqdm(enumerate(test_loder), bar_format='{l_bar}{bar}| Event {n_fmt}/{total_fmt} [{elapsed}<{remaining}, ' '{rate_fmt}{postfix}]' , total = len(test_loder) , ncols = 75 ) :  \n",
    "\n",
    "        output = model(graph_data)\n",
    "        \n",
    "        loss_fn = nn.BCEWithLogitsLoss(\n",
    "            reduction='sum' , pos_weight=graph_data.edge_purity/( 1-graph_data.edge_purity)\n",
    "        )\n",
    "        loss = loss_fn(output,graph_data.edge_attr)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        if( i == 2 ) : \n",
    "            break\n",
    "        \n",
    "    return train_loss/num_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TrainFiltering(filter_model_instance,train_loder_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TestEmbedding(filter_model_instance,val_loder_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrackML.Filtering.utils import PostEmbeddingGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule\n",
    "from torchmetrics import Metric\n",
    "import yaml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('04-Filtering-Hyperparameters.yml' , 'r' ) as f : \n",
    "    hparams = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilteringDataset(pl.LightningDataModule): \n",
    "    def __init__(self,hparams:dict)->None: \n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "        \n",
    "    def setup(self,stage=None): \n",
    "        dataset = PostEmbeddingGraph(\n",
    "            dataset_path=self.hparams['dataset_path'], \n",
    "            detector_path=self.hparams['detector_path'], \n",
    "            embd_model_path=self.hparams['embd_model_path'], \n",
    "            min_nhits = self.hparams['min_nhits'], \n",
    "            margin = self.hparams['margin'] , \n",
    "            max_num_neighbors= self.hparams['max_num_neighbours']\n",
    "        )\n",
    "        self.train_ds , self.val_ds , self.test_ds = train_test_split(\n",
    "            dataset=dataset, \n",
    "            valid_size=self.hparams['valid_size'], \n",
    "            test_size=self.hparams['test_size'] , \n",
    "            num_works=self.hparams['num_works']\n",
    "        )\n",
    "    \n",
    "    def train_dataloader(self): \n",
    "        return self.train_ds \n",
    "    def val_dataloader(self): \n",
    "        return self.val_ds \n",
    "    def test_dataloader(self): \n",
    "        return self.test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification import BinaryPrecision, BinaryRecall, BinaryF1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogSumLoss(Metric):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.add_state(\"log_losses\", default=torch.tensor([]), dist_reduce_fx=\"cat\")\n",
    "\n",
    "    def update(self, loss):\n",
    "        # Expect loss to be a tensor of shape (N,) or scalar\n",
    "        loss = loss.flatten().detach()\n",
    "        self.log_losses = torch.cat([self.log_losses, torch.log(loss)])\n",
    "\n",
    "    def compute(self):\n",
    "        # log-sum-exp: log(sum(exp(x))) = max + log(sum(exp(x - max)))\n",
    "        if self.log_losses.numel() == 0:\n",
    "            return torch.tensor(float(\"-inf\"), device=self.log_losses.device)\n",
    "\n",
    "        max_log = torch.max(self.log_losses)\n",
    "        return max_log + torch.log(torch.sum(torch.exp(self.log_losses - max_log)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilteringModelPl(LightningModule): \n",
    "    \n",
    "    def __init__(self,hparams): \n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "        \n",
    "        # Metrics (with DDP support)\n",
    "        self.train_precision = BinaryPrecision()\n",
    "        self.train_recall = BinaryRecall()\n",
    "        self.train_f1 = BinaryF1Score()\n",
    "\n",
    "        self.val_precision = BinaryPrecision()\n",
    "        self.val_recall = BinaryRecall()\n",
    "        self.val_f1 = BinaryF1Score()\n",
    "        \n",
    "        self.log_sum_loss = LogSumLoss()\n",
    "        \n",
    "        self.model = FilteringModel(\n",
    "            in_features=hparams['in_featuers'],\n",
    "            hidden_features=hparams['hidden_features']\n",
    "        )\n",
    "        \n",
    "    def forward(self , x ): \n",
    "        return self.model( x )\n",
    "    \n",
    "    def training_step(self, batch , batch_idx ):\n",
    "        print( batch )\n",
    "        output = self( batch )\n",
    "        \n",
    "        loss_fn = nn.BCEWithLogitsLoss(\n",
    "            reduction='sum' , pos_weight=batch.edge_purity/( 1-batch.edge_purity)\n",
    "        )\n",
    "        loss = loss_fn(output,batch.edge_attr)\n",
    "        \n",
    "        with torch.no_grad() : \n",
    "            preds = torch.nn.Sigmoid()(output)\n",
    "            self.train_precision.update( preds , batch.edge_attr )\n",
    "            self.train_recall.update(preds, batch.edge_attr)\n",
    "            self.train_f1.update(preds, batch.edge_attr)\n",
    "            \n",
    "        if torch.cuda.is_available(): \n",
    "                self.log(\n",
    "                    'Memory Allocated' , torch.cuda.memory_allocated()/(1024**3), \n",
    "                    prog_bar=True , on_step = True , on_epoch=True, \n",
    "                    reduce_fx='max' , sync_dist=True\n",
    "                )\n",
    "        \n",
    "        self.log('loss' , loss , on_step = True , on_epoch = False , prog_bar=True )\n",
    "        self.last_idx = batch_idx\n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        self.log(\"train_precision\", self.train_precision.compute(), prog_bar=True,sync_dist=True)\n",
    "        self.log(\"train_recall\", self.train_recall.compute(), prog_bar=True ,sync_dist=True)\n",
    "        self.log(\"train_f1\", self.train_f1.compute(), prog_bar=True,sync_dist=True)\n",
    "        \n",
    "        self.train_precision.reset()\n",
    "        self.train_recall.reset()\n",
    "        self.train_f1.reset()\n",
    "        \n",
    "        if self.last_idx % 10 == 0 and torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect() \n",
    "    \n",
    "    def  _test_val_common_step_( self , batch , batch_idx ): \n",
    "        with torch.no_grad() : \n",
    "            output = self( batch )\n",
    "        \n",
    "            loss_fn = nn.BCEWithLogitsLoss(\n",
    "                reduction='sum' , pos_weight=batch.edge_purity/( 1-batch.edge_purity)\n",
    "            )\n",
    "            loss = loss_fn(output,batch.edge_attr)\n",
    "            self.log_sum_loss.update( loss )\n",
    "            self.log('loss' , loss , on_step = True , on_epoch = False , prog_bar=True )     \n",
    "            \n",
    "            if torch.cuda.is_available(): \n",
    "                self.log(\n",
    "                    'Memory Allocated' , torch.cuda.memory_allocated()/(1024**3), \n",
    "                    prog_bar=True , on_step = True , on_epoch=True, \n",
    "                    reduce_fx='max' , sync_dist=True\n",
    "                )       \n",
    "            \n",
    "            preds = torch.nn.Sigmoid()(output)\n",
    "            \n",
    "            self.val_precision.update(preds, batch.edge_attr)\n",
    "            self.val_recall.update(preds, batch.edge_attr)\n",
    "            self.val_f1.update(preds, batch.edge_attr)\n",
    "            \n",
    "            self.last_idx = batch_idx\n",
    "            \n",
    "            return loss \n",
    "\n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        return self._test_val_common_step_(batch,batch_idx)\n",
    "\n",
    "    def test_step(self,batch,batch_idx): \n",
    "        return self._test_val_common_step_(batch,batch_idx)\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        self.log(\"val_precision\", self.val_precision.compute(), prog_bar=True ,sync_dist=True)\n",
    "        self.log(\"val_recall\", self.val_recall.compute(), prog_bar=True ,sync_dist=True)\n",
    "        self.log(\"val_f1\", self.val_f1.compute(), prog_bar=True ,sync_dist=True)\n",
    "\n",
    "        self.val_precision.reset()\n",
    "        self.val_recall.reset()\n",
    "        self.val_f1.reset()\n",
    "        self.log_sum_loss.reset()\n",
    "        \n",
    "        if self.last_idx % 10 == 0 and torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "        \n",
    "    def configure_optimizers(self): \n",
    "        return torch.optim.SGD(self.model.parameters(),lr = self.hparams['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/ashmitbathla/Documents/UGP-Local/TrackML/TrackMLVenv/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n"
     ]
    }
   ],
   "source": [
    "model = FilteringModelPl(hparams)\n",
    "ds = FilteringDataset(hparams)\n",
    "\n",
    "from pytorch_lightning.callbacks import DeviceStatsMonitor\n",
    "device_stats = DeviceStatsMonitor()\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator = \"cpu\", \n",
    "    devices = \"auto\",\n",
    "    enable_checkpointing=False, \n",
    "    fast_dev_run = 1, \n",
    "    callbacks=[device_stats]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name            | Type            | Params | Mode \n",
      "------------------------------------------------------------\n",
      "0 | train_precision | BinaryPrecision | 0      | train\n",
      "1 | train_recall    | BinaryRecall    | 0      | train\n",
      "2 | train_f1        | BinaryF1Score   | 0      | train\n",
      "3 | val_precision   | BinaryPrecision | 0      | train\n",
      "4 | val_recall      | BinaryRecall    | 0      | train\n",
      "5 | val_f1          | BinaryF1Score   | 0      | train\n",
      "6 | log_sum_loss    | LogSumLoss      | 0      | train\n",
      "7 | model           | FilteringModel  | 951    | train\n",
      "------------------------------------------------------------\n",
      "951       Trainable params\n",
      "0         Non-trainable params\n",
      "951       Total params\n",
      "0.004     Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/Users/ashmitbathla/Documents/UGP-Local/TrackML/TrackMLVenv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/ashmitbathla/Documents/UGP-Local/TrackML/TrackMLVenv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09cae74f8ac244c3bb4b3b9e880d5e0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[116980, 15], edge_index=[2, 11698000], edge_attr=[11698000, 1], y=[116980], edge_purity=[1], batch=[116980], ptr=[2])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7359fb27c7114aa9a62a6425fc5dab45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=1` reached.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    trainer.fit(model , ds )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TrackMLVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
